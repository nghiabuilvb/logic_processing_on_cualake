{
  "metadata": {
    "name": "orders Copy",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# Connection Properties\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ndBHost \u003d \"168.138.196.123\"\ndBPort \u003d \"3306\"\ndBSID \u003d \"\"\ndBName \u003d \"inventory\"\ndBUsername \u003d \"root\"\ndBPassword \u003d \"debezium\"\ndBQuery \u003d \"\"\"SELECT * FROM orders\"\"\"\nprimaryColumnName \u003d \"order_number\"\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nif \"MySQL\"\u003d\u003d\"Oracle\":\n    df \u003d spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", f\"jdbc:oracle:thin:@{dBHost}:{dBPort}/{dBSID}\") \\\n        .option(\"query\", dBQuery) \\\n        .option(\"password\", dBPassword) \\\n        .option(\"user\", dBUsername) \\\n        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        .load()\n        \nelif \"MySQL\"\u003d\u003d\"Postgres\":\n    df \u003d spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", f\"jdbc:postgresql://{dBHost}:{dBPort}/{dBName}\") \\\n        .option(\"query\", dBQuery) \\\n        .option(\"password\", dBPassword) \\\n        .option(\"user\", dBUsername) \\\n        .option(\"driver\", \"org.postgresql.Driver\")\n    if \"\" \u003d\u003d \"True\":\n        df \u003d df.option(\"sslmode\", \"require\")\n    df \u003d df.load()\n    \nelif \"MySQL\"\u003d\u003d\"MySQL\":\n    df \u003d spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", f\"jdbc:mysql://{dBHost}:{dBPort}/{dBName}\") \\\n        .option(\"query\", dBQuery) \\\n        .option(\"password\", dBPassword) \\\n        .option(\"user\", dBUsername)\n    if \"\" \u003d\u003d \"True\":\n        df \u003d df.option(\"sslmode\", \"require\")\n    df \u003d df.load()\n        \n# Set order by column for SortOrder\ndf \u003d df.orderBy(\"order_number\")\n\n# Store results in temp table\ndf.createOrReplaceTempView(\"tempTable_1634011621426\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- Create table if not exist\nCREATE TABLE IF NOT EXISTS cuelake.orders USING iceberg \n  TBLPROPERTIES (\n        \u0027write.metadata.delete-after-commit.enabled\u0027\u003d \u0027true\u0027,\n       \u0027write.metadata.previous-versions-max\u0027 \u003d 2,\n       \u0027history.expire.max-snapshot-age-ms\u0027 \u003d \u002760000\u0027,\n       \u0027history.expire.min-snapshots-to-keep\u0027 \u003d 1,\n       \u0027write.spark.fanout.enabled\u0027 \u003d \u0027true\u0027,\n        \u0027write.metadata.metrics.default\u0027 \u003d \u0027none\u0027\n   )\n  AS SELECT /*+ COALESCE(1) */ * from tempTable_1634011621426"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval tableName \u003d \"orders\"\n\nimport org.apache.iceberg.actions.Actions;\nimport org.apache.iceberg.hadoop.HadoopCatalog;\nval catalog \u003d new HadoopCatalog(sc.hadoopConfiguration, sc.getConf.get(\"spark.sql.catalog.cuelake.warehouse\"));\n\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.catalog.TableIdentifier;\n\nval name \u003d TableIdentifier.of(tableName);\nval table \u003d catalog.loadTable(name);\n\n// Expire older snapshots and commit\ntable.expireSnapshots().commit()\n\n// Run Compaction for table\nActions.forTable(table).rewriteDataFiles()\n    .targetSizeInBytes(500 * 1024 * 1024 * 10) // 5000 MB\n    .execute();"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmaxVal\u003dspark.sql(f\"SELECT MAX(order_date) FROM cuelake.orders\").collect()[0][0]\nz.put(\"tempTable_1634011621426_val\", maxVal)\nprint(\"Timestamp value in target table: \" + str(maxVal))"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nselect * from  cuelake.orders;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- SQL Query to configure sort order\nALTER TABLE cuelake.orders WRITE ORDERED BY order_number;"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmergeSql \u003d f\"MERGE INTO cuelake.orders t USING (SELECT * from tempTable_1634011621426 where order_date \u003e \\\"{z.get(\u0027tempTable_1634011621426_val\u0027)}\\\") u ON t.order_number \u003d u.order_number WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *\"\nprint(\"Executing SQL: \" + mergeSql)\nspark.sql(mergeSql)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}